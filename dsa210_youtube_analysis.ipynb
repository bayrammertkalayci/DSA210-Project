{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAUFY_QHlQSv"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas google-api-python-client isodate tqdm seaborn matplotlib scipy numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRXdHo4ile0y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "FILE_NAME = 'watch-history.json'\n",
        "video_data = []\n",
        "\n",
        "try:\n",
        "    with open(FILE_NAME, 'r', encoding='utf-8') as f:\n",
        "        history = json.load(f)\n",
        "except Exception as e:\n",
        "    # Set history to empty list on failure\n",
        "    history = []\n",
        "\n",
        "for item in history:\n",
        "    # Filter only for video watch records with a valid YouTube watch URL\n",
        "    if 'titleUrl' in item and 'time' in item and 'youtube.com/watch?v=' in item.get('titleUrl', ''):\n",
        "        try:\n",
        "            # Extract video ID from the URL\n",
        "            video_id = item['titleUrl'].split('v=')[-1].split('&')[0]\n",
        "\n",
        "            # Collect essential data\n",
        "            video_data.append({\n",
        "                'video_ID': video_id,\n",
        "                'watch_timestamp': item['time'],\n",
        "                'video_title': item['title'].replace('Watched ', '').strip()\n",
        "            })\n",
        "        except Exception:\n",
        "            # Skip records with faulty/missing data format\n",
        "            continue\n",
        "\n",
        "df_takeout = pd.DataFrame(video_data)\n",
        "# Output only the size of the dataframe\n",
        "print(len(df_takeout))\n",
        "# Output the first 5 records\n",
        "print(df_takeout.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynyIxzYDnfF3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import os # Import os module to access environment variables\n",
        "\n",
        "# --- API KEY HANDLING: Securely retrieve the key from environment variables ---\n",
        "API_KEY = os.environ.get(\"YOUTUBE_API_KEY\") \n",
        "\n",
        "if not API_KEY:\n",
        "    # If the key is not set in the environment, skip API calls.\n",
        "    df_api = pd.DataFrame()\n",
        "    # Ensure df_merged is created so subsequent cells don't fail\n",
        "    df_merged = df_takeout.copy() \n",
        "    print(\"API_KEY not found in environment variables (YOUTUBE_API_KEY). Skipping API data fetch.\")\n",
        "    print(len(df_merged))\n",
        "    print(df_merged.head())\n",
        "    \n",
        "else:\n",
        "    # YouTube API client initialization\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "    \n",
        "    unique_video_ids = df_takeout['video_ID'].unique()\n",
        "    print(len(unique_video_ids))\n",
        "\n",
        "    api_results = []\n",
        "    chunk_size = 50 \n",
        "\n",
        "    # Query IDs in chunks and display progress\n",
        "    for i in tqdm(range(0, len(unique_video_ids), chunk_size), desc=\"API Query Progress\"):\n",
        "        chunk = unique_video_ids[i:i + chunk_size]\n",
        "\n",
        "        try:\n",
        "            request = youtube.videos().list(\n",
        "                part=\"snippet,contentDetails,statistics\",\n",
        "                id=','.join(chunk)\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response.get('items', []):\n",
        "                stats = item.get('statistics', {})\n",
        "                content = item.get('contentDetails', {})\n",
        "                snippet = item.get('snippet', {})\n",
        "\n",
        "                api_results.append({\n",
        "                    'video_ID': item['id'],\n",
        "                    'categoryName': snippet.get('categoryId'),\n",
        "                    'viewCount': int(stats.get('viewCount', 0)),\n",
        "                    'likeCount': int(stats.get('likeCount', 0)),\n",
        "                    'duration_raw': content.get('duration')\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            # Skip all error/quota warnings\n",
        "            break\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    df_api = pd.DataFrame(api_results)\n",
        "\n",
        "    # Map YouTube category IDs to descriptive names\n",
        "    category_map = {\n",
        "        \"1\": \"Film & Animation\", \"2\": \"Autos & Vehicles\", \"10\": \"Music\", \"15\": \"Pets & Animals\",\n",
        "        \"17\": \"Sports\", \"19\": \"Travel & Events\", \"20\": \"Gaming\", \"22\": \"People & Blogs\",\n",
        "        \"23\": \"Comedy\", \"24\": \"Entertainment\", \"25\": \"News & Politics\", \"26\": \"Howto & Style\",\n",
        "        \"27\": \"Education\", \"28\": \"Science & Technology\", \"29\": \"Nonprofits & Activism\",\n",
        "        \"30\": \"Movies\", \"43\": \"Shows\"\n",
        "    }\n",
        "    df_api['categoryName'] = df_api['categoryName'].astype(str).map(category_map).fillna('Other')\n",
        "\n",
        "    # Merge the two dataframes\n",
        "    df_merged = pd.merge(df_takeout, df_api, on='video_ID', how='inner')\n",
        "\n",
        "    # Output only the final record count and the first 5 records\n",
        "    print(len(df_merged))\n",
        "    print(df_merged.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOrqFXEe-8EX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import isodate\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Convert watch timestamp to datetime object with correct UTC and format handling\n",
        "df_merged['watch_timestamp'] = pd.to_datetime(df_merged['watch_timestamp'], utc=True, format='mixed')\n",
        "\n",
        "# Extract time-based features\n",
        "df_merged['hour_of_day'] = df_merged['watch_timestamp'].dt.hour\n",
        "df_merged['day_of_week'] = df_merged['watch_timestamp'].dt.day_name()\n",
        "df_merged['is_weekend'] = df_merged['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
        "\n",
        "# Convert days to an ordered categorical type (useful for plotting)\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "df_merged['day_of_week'] = pd.Categorical(df_merged['day_of_week'], categories=day_order, ordered=True)\n",
        "\n",
        "def convert_iso_to_seconds(iso_duration):\n",
        "    \"\"\"Helper function to convert ISO 8601 duration to seconds.\"\"\"\n",
        "    try:\n",
        "        return isodate.parse_duration(iso_duration).total_seconds()\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Convert video durations to seconds. Add safety check for missing API columns.\n",
        "if 'duration_raw' in df_merged.columns:\n",
        "    df_merged['duration_seconds'] = df_merged['duration_raw'].apply(convert_iso_to_seconds)\n",
        "else:\n",
        "    df_merged['duration_seconds'] = 0 \n",
        "\n",
        "# Apply log transformation to view and like counts. Add safety check for missing API columns.\n",
        "if 'viewCount' in df_merged.columns:\n",
        "    df_merged['log_viewCount'] = np.log1p(df_merged['viewCount'])\n",
        "    df_merged['log_likeCount'] = np.log1p(df_merged['likeCount'])\n",
        "else:\n",
        "    df_merged['log_viewCount'] = np.nan\n",
        "    df_merged['log_likeCount'] = np.nan\n",
        "\n",
        "# Create final clean dataframe by dropping rows with missing essential data\n",
        "df_final = df_merged.dropna(subset=['log_viewCount', 'categoryName', 'hour_of_day']).copy()\n",
        "\n",
        "# Output only the descriptive statistics\n",
        "print(df_final[['hour_of_day', 'day_of_week', 'log_viewCount', 'duration_seconds']].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz_category_count"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "\n",
        "if not df_final.empty:\n",
        "    # Bar Plot: Total Watch Count by Category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    category_counts_plot = df_final['categoryName'].value_counts().sort_values(ascending=False)\n",
        "    sns.barplot(x=category_counts_plot.index, y=category_counts_plot.values, palette=\"viridis\")\n",
        "    plt.title('Total Watch Count by Video Category')\n",
        "    plt.xlabel('Video Category')\n",
        "    plt.ylabel('Watch Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping Category Count Visualization: Dataframe is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz_hour_count"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "\n",
        "if not df_final.empty:\n",
        "    # Bar Plot: Watch Count by Hour of Day\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    hour_counts = df_final['hour_of_day'].value_counts().sort_index()\n",
        "    sns.barplot(x=hour_counts.index, y=hour_counts.values, color='skyblue')\n",
        "    plt.title('Viewing Frequency by Hour of Day')\n",
        "    plt.xlabel('Hour of Day (24h)')\n",
        "    plt.ylabel('Watch Count')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping Hour Count Visualization: Dataframe is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQDYBu3N-_bu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency, f_oneway, pearsonr\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "\n",
        "# Check if df_final is empty (which happens if API fetch was skipped and dropped NaN rows)\n",
        "if df_final.empty:\n",
        "    print(\"Warning: df_final is empty (likely due to missing API data). Skipping visualizations and hypothesis tests.\")\n",
        "    \n",
        "else:\n",
        "    # Heatmap: Viewing Density (Personal Consumption Pattern)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # The observed=False prevents the removal of unused categories, keeping the structure consistent.\n",
        "    heatmap_data = df_final.groupby(['day_of_week', 'hour_of_day'], observed=False).size().unstack(fill_value=0)\n",
        "    sns.heatmap(heatmap_data, cmap=\"YlGnBu\", linewidths=.5, cbar_kws={'label': 'Watch Count'})\n",
        "    plt.title('Viewing Density by Day of Week and Hour (Personal Patterns)')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Day of Week')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Box Plot: View Count Distribution for Top Categories\n",
        "    top_categories = df_final['categoryName'].value_counts().head(7).index\n",
        "    df_top = df_final[df_final['categoryName'].isin(top_categories)]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # The 'y' variable is passed as 'hue' to suppress the deprecation warning without changing the plot logic\n",
        "    sns.boxplot(x='log_viewCount', y='categoryName', data=df_top, palette=\"Pastel1\", hue='categoryName', legend=False)\n",
        "    plt.title('Distribution of Log(View Count) in Top Categories')\n",
        "    plt.xlabel('Log(1 + View Count)')\n",
        "    plt.ylabel('Video Category')\n",
        "    plt.show()\n",
        "\n",
        "    # Filter categories with at least 10 views for robust hypothesis testing\n",
        "    category_counts = df_final['categoryName'].value_counts()\n",
        "    valid_categories = category_counts[category_counts >= 10].index\n",
        "    df_test = df_final[df_final['categoryName'].isin(valid_categories)].copy()\n",
        "\n",
        "    # --- HYPOTHESIS TEST BLOCK 1: Chi-Square (Time vs Category) ---\n",
        "    print(\"\\n--- HYPOTHESIS TEST 1: Time vs Category ---\")\n",
        "    contingency_table = pd.crosstab(df_test['day_of_week'], df_test['categoryName'])\n",
        "    \n",
        "    # Need to ensure the table isn't trivial before Chi-Square\n",
        "    if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
        "        chi2, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
        "        print(f\"P-Value: {p_chi2:.10f}\")\n",
        "        print(\"Conclusion: \" + (\"Relationship exists (p < 0.05).\" if p_chi2 < 0.05 else \"No relationship found (p >= 0.05).\"))\n",
        "    else:\n",
        "        print(\"Test Skipped: Insufficient data for Chi-Square test.\")\n",
        "\n",
        "\n",
        "    # --- HYPOTHESIS TEST BLOCK 2: ANOVA (Category Popularity Difference) ---\n",
        "    print(\"\\n--- HYPOTHESIS TEST 2: Category Popularity ---\")\n",
        "    anova_groups = [df_test['log_viewCount'][df_test['categoryName'] == cat].values\n",
        "                    for cat in valid_categories if len(df_test['log_viewCount'][df_test['categoryName'] == cat]) > 0]\n",
        "    \n",
        "    # Need to ensure we have at least two groups for ANOVA\n",
        "    if len(anova_groups) >= 2:\n",
        "        f_stat, p_anova = f_oneway(*anova_groups)\n",
        "        print(f\"P-Value: {p_anova:.10f}\")\n",
        "        print(\"Conclusion: \" + (\"Significant difference exists (p < 0.05).\" if p_anova < 0.05 else \"No significant difference found (p >= 0.05).\"))\n",
        "    else:\n",
        "        print(\"Test Skipped: Less than two valid categories for ANOVA test.\")\n",
        "\n",
        "\n",
        "    # --- HYPOTHESIS TEST BLOCK 3: Pearson Correlation (Time vs Popularity) ---\n",
        "    print(\"\\n--- HYPOTHESIS TEST 3: Time vs Popularity ---\")\n",
        "    # Check for sufficient data points\n",
        "    if len(df_test) > 1:\n",
        "        corr, p_corr = pearsonr(df_test['hour_of_day'], df_test['log_viewCount'])\n",
        "        print(f\"Correlation Coefficient (r): {corr:.4f}\")\n",
        "        print(f\"P-Value: {p_corr:.10f}\")\n",
        "        print(\"Conclusion: \" + (\"Relationship exists (p < 0.05).\" if p_corr < 0.05 else \"No relationship found (p >= 0.05).\"))\n",
        "    else:\n",
        "        print(\"Test Skipped: Insufficient data for Pearson correlation test.\")"
      ]
    }
  ]
}
